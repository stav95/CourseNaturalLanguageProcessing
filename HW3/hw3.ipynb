{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join as pj\n",
    "from random import shuffle\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_FOLDER = pj('thesis', 'NLP_Course', 'HW3', 'data')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def read_data(filename: str) -> List[List[str]]:\n",
    "    data = []\n",
    "\n",
    "    with open(pj(DATA_FOLDER, filename), 'r') as f:\n",
    "        output = f.read().splitlines()\n",
    "\n",
    "    words, tags = [], []\n",
    "    for line in output:\n",
    "        if line == '':\n",
    "            data.append((words, tags))\n",
    "            words, tags = [], []\n",
    "            continue\n",
    "\n",
    "        line_splitted = line.strip().split(' ')\n",
    "        words.append(line_splitted[0])\n",
    "        tags.append(line_splitted[1])\n",
    "\n",
    "    if len(words) > 0:\n",
    "        data.append((words, tags))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "_train, _dev, _test = read_data('connl03_train.txt'), read_data('connl03_dev.txt'), read_data('connl03_test.txt')\n",
    "\n",
    "UNK_TOKEN = 0\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2id: Dict[str, int] = {\"__unk__\": UNK_TOKEN}\n",
    "        self.id2word: Dict[int, str] = {UNK_TOKEN: \"__unk__\"}\n",
    "        self.n_words = 1\n",
    "\n",
    "        self.tag2id: Dict[str, int] = {\n",
    "            \"O\": 0,\n",
    "            \"B-PER\": 1,\n",
    "            \"I-PER\": 2,\n",
    "            \"B-LOC\": 3,\n",
    "            \"I-LOC\": 4,\n",
    "            \"B-ORG\": 5,\n",
    "            \"I-ORG\": 6\n",
    "        }\n",
    "        self.id2tag: Dict[int, str] = {\n",
    "            0: \"O\",\n",
    "            1: \"B-PER\",\n",
    "            2: \"I-PER\",\n",
    "            3: \"B-LOC\",\n",
    "            4: \"I-LOC\",\n",
    "            5: \"B-ORG\",\n",
    "            6: \"I-ORG\"\n",
    "        }\n",
    "\n",
    "    def index_words(self, words: List[str]) -> List[int]:\n",
    "        word_indexes = [self.index_word(w) for w in words]\n",
    "        return word_indexes\n",
    "\n",
    "    def index_tags(self, tags: List[str]) -> List[int]:\n",
    "        tag_indexes = [self.tag2id[t] for t in tags]\n",
    "        return tag_indexes\n",
    "\n",
    "    def index_word(self, w: str) -> int:\n",
    "        if w not in self.word2id:\n",
    "            self.word2id[w] = self.n_words\n",
    "            self.id2word[self.n_words] = w\n",
    "            self.n_words += 1\n",
    "        return self.word2id[w]\n",
    "\n",
    "\n",
    "_vocab = Vocab()\n",
    "\n",
    "\n",
    "def prepare_data(data: List[Tuple[List[str], List[str]]],\n",
    "                 vocab: Vocab) -> Tuple[List[Tuple[torch.LongTensor, torch.LongTensor]], Vocab]:\n",
    "    data_sequences = []\n",
    "\n",
    "    for words, tags in data:\n",
    "        words_indexes = torch.LongTensor(vocab.index_words(words))\n",
    "        tags_indexes = torch.LongTensor(vocab.index_tags(tags))\n",
    "\n",
    "        data_sequences.append((words_indexes, tags_indexes))\n",
    "\n",
    "    return data_sequences, vocab\n",
    "\n",
    "\n",
    "_train_sequences, _vocab = prepare_data(data=_train, vocab=_vocab)\n",
    "_dev_sequences, _vocab = prepare_data(data=_dev, vocab=_vocab)\n",
    "_test_sequences, _vocab = prepare_data(data=_test, vocab=_vocab)\n",
    "\n",
    "\n",
    "class NERNet(nn.Module):\n",
    "    def __init__(self, input_size: int, embedding_size: int, hidden_size: int, output_size: int,\n",
    "                 n_layers: int, directions: int):\n",
    "        super(NERNet, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.directions = directions\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        # bidirectional if directions==2 else 1\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=(directions == 2))\n",
    "        self.out = nn.Linear(in_features=hidden_size * directions, out_features=output_size)\n",
    "\n",
    "    def forward(self, input_sentence: torch.LongTensor) -> torch.Tensor:\n",
    "        embeds = self.embedding(input_sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(input_sentence.shape[0], 1, -1))\n",
    "        output = self.out(lstm_out.view(input_sentence.shape[0], -1))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_loop(model: NERNet, n_epochs: int, train_sequences: List[Tuple[torch.LongTensor, torch.LongTensor]]):\n",
    "    shuffle(train_sequences)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    sentence_gpu, tags_gpu = [], []\n",
    "    for sentence, tags in train_sequences:\n",
    "        sentence_gpu.append(sentence.to(DEVICE))\n",
    "        tags_gpu.append(tags.to(DEVICE))\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for sentence, tags in zip(sentence_gpu, tags_gpu):\n",
    "            model.zero_grad()\n",
    "            scores = model(sentence)\n",
    "            criterion(scores, tags).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model: NERNet, caption: str, vocab: Vocab, dev_sequences: List[Tuple[torch.LongTensor, torch.LongTensor]]\n",
    "             , test_sequences: List[Tuple[torch.LongTensor, torch.LongTensor]]):\n",
    "    n_labels = len(vocab.tag2id)\n",
    "\n",
    "    dev_matrix = torch.zeros(n_labels, n_labels, dtype=torch.float32)\n",
    "    test_matrix = torch.zeros(n_labels, n_labels, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_sequences:\n",
    "            preds = model(inputs.to(DEVICE)).max(1).indices\n",
    "            for label, pred in zip(labels, preds):\n",
    "                dev_matrix[label, pred] += 1\n",
    "\n",
    "        for inputs, labels in test_sequences:\n",
    "            preds = model(inputs.to(DEVICE)).max(1).indices\n",
    "            for label, pred in zip(labels, preds):\n",
    "                test_matrix[label, pred] += 1\n",
    "\n",
    "    # Calculate Precision.\n",
    "    dev_precision = dev_matrix.diag() / dev_matrix.sum(1)\n",
    "    test_precision = test_matrix.diag() / test_matrix.sum(1)\n",
    "\n",
    "    # Calculate Recall.\n",
    "    dev_recall = dev_matrix.diag() / dev_matrix.sum(0)\n",
    "    test_recall = test_matrix.diag() / test_matrix.sum(0)\n",
    "\n",
    "    ndigits = 3  # For displaying purposes.\n",
    "    df = pd.DataFrame(columns=vocab.tag2id.keys())\n",
    "\n",
    "    df.loc['dev, Precision'] = [round(x, ndigits) for x in dev_precision.tolist()]\n",
    "    df.loc['test, Precision'] = [round(x, ndigits) for x in test_precision.tolist()]\n",
    "\n",
    "    df.loc['dev, Recall'] = [round(x, ndigits) for x in dev_recall.tolist()]\n",
    "    df.loc['test, Recall'] = [round(x, ndigits) for x in test_recall.tolist()]\n",
    "\n",
    "    # Add labels.\n",
    "    dev_precision_except_0 = dev_matrix[1:, 1:].diag().sum() / dev_matrix[1:, 1:].sum(1).sum()\n",
    "    test_precision_except_0 = test_matrix[1:, 1:].diag().sum() / test_matrix[1:, 1:].sum(1).sum()\n",
    "\n",
    "    dev_recall_except_0 = dev_matrix[1:, 1:].diag().sum() / dev_matrix[1:, 1:].sum(0).sum()\n",
    "    test_recall_except_0 = test_matrix[1:, 1:].diag().sum() / test_matrix[1:, 1:].sum(0).sum()\n",
    "\n",
    "    df.loc['dev, Precision', 'All Except O'] = round(float(dev_precision_except_0), ndigits)\n",
    "    df.loc['test, Precision', 'All Except O'] = round(float(test_precision_except_0), ndigits)\n",
    "\n",
    "    df.loc['dev, Recall', 'All Except O'] = round(float(dev_recall_except_0), ndigits)\n",
    "    df.loc['test, ''Recall', 'All Except O'] = round(float(test_recall_except_0), ndigits)\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    print(caption)\n",
    "    print(df)\n",
    "    print()\n",
    "\n",
    "\n",
    "def get_glove_weights(vocab: Vocab) -> torch.FloatTensor:\n",
    "    with open(pj(DATA_FOLDER, 'glove.6B.300d.txt'), encoding='utf-8') as f:\n",
    "        embeddings = torch.zeros((len(vocab.word2id), 300), dtype=torch.float32)\n",
    "        for line in f.readlines():\n",
    "            vals = line.split()\n",
    "            idx = vocab.word2id.get(vals[0])\n",
    "            if idx:\n",
    "                embeddings[idx] = torch.FloatTensor([float(x) for x in vals[1:]])\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def main(vocab: Vocab, load_glove_weights: bool):\n",
    "    if load_glove_weights:\n",
    "        embedding_weights = get_glove_weights(vocab=_vocab)\n",
    "    else:\n",
    "        embedding_weights = None\n",
    "\n",
    "    model_i = 0\n",
    "    for hidden_size in [500, 800]:\n",
    "        for n_layers in [1, 2, 3]:\n",
    "            for directions in [1, 2] if hidden_size == 500 else [2]:\n",
    "                model = NERNet(input_size=len(vocab.word2id), embedding_size=300,\n",
    "                               hidden_size=hidden_size, output_size=len(vocab.tag2id),\n",
    "                               n_layers=n_layers, directions=directions)\n",
    "\n",
    "                if embedding_weights is not None:\n",
    "                    model.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
    "\n",
    "                model.to(DEVICE)\n",
    "\n",
    "                train_loop(model, n_epochs=10, train_sequences=_train_sequences)\n",
    "                model_i += 1\n",
    "                caption = (f'Model {model_i} {\"(GloVe)\" if embedding_weights is not None else \"\"},'\n",
    "                           f' hidden_size: {model.hidden_size}, n_layers: {model.n_layers},'\n",
    "                           f' directions: {model.directions})')\n",
    "\n",
    "                evaluate(model, caption, vocab=_vocab, dev_sequences=_dev_sequences, test_sequences=_test_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 , hidden_size: 500, n_layers: 1, directions: 1)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.957  0.645  0.637  0.678  0.522  0.625  0.440         0.865\n",
      "test, Precision  0.954  0.636  0.703  0.717  0.509  0.557  0.350         0.847\n",
      "dev, Recall      0.924  0.713  0.800  0.800  0.800  0.571  0.671         0.865\n",
      "test, Recall     0.930  0.706  0.819  0.786  0.931  0.509  0.504         0.847\n",
      "\n",
      "Model 2 , hidden_size: 500, n_layers: 1, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.972  0.710  0.688  0.770  0.435  0.613  0.397         0.862\n",
      "test, Precision  0.973  0.694  0.655  0.764  0.585  0.597  0.390         0.889\n",
      "dev, Recall      0.935  0.789  0.871  0.731  1.000  0.669  0.730         0.862\n",
      "test, Recall     0.932  0.768  0.878  0.787  0.912  0.701  0.729         0.889\n",
      "\n",
      "Model 3 , hidden_size: 500, n_layers: 2, directions: 1)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.973  0.685  0.732  0.754  0.478  0.577  0.431         0.863\n",
      "test, Precision  0.966  0.710  0.706  0.697  0.415  0.563  0.370         0.835\n",
      "dev, Recall      0.934  0.703  0.852  0.836  0.917  0.642  0.820         0.863\n",
      "test, Recall     0.938  0.665  0.739  0.875  0.957  0.625  0.612         0.835\n",
      "\n",
      "Model 4 , hidden_size: 500, n_layers: 2, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.984  0.705  0.752  0.776  0.565  0.637  0.422         0.893\n",
      "test, Precision  0.979  0.735  0.753  0.787  0.604  0.611  0.425         0.907\n",
      "dev, Recall      0.936  0.829  0.874  0.840  0.765  0.759  0.875         0.893\n",
      "test, Recall     0.939  0.796  0.829  0.860  0.889  0.796  0.794         0.907\n",
      "\n",
      "Model 5 , hidden_size: 500, n_layers: 3, directions: 1)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.972  0.635  0.694  0.672  0.478  0.554  0.397         0.851\n",
      "test, Precision  0.969  0.675  0.709  0.685  0.491  0.594  0.470         0.869\n",
      "dev, Recall      0.924  0.690  0.832  0.911  0.647  0.620  0.676         0.851\n",
      "test, Recall     0.934  0.771  0.811  0.897  0.839  0.640  0.537         0.869\n",
      "\n",
      "Model 6 , hidden_size: 500, n_layers: 3, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.968  0.745  0.790  0.787  0.522  0.720  0.560         0.903\n",
      "test, Precision  0.968  0.696  0.767  0.781  0.566  0.654  0.540         0.854\n",
      "dev, Recall      0.948  0.903  0.838  0.774  0.800  0.703  0.691         0.903\n",
      "test, Recall     0.953  0.863  0.832  0.713  0.811  0.658  0.571         0.854\n",
      "\n",
      "Model 7 , hidden_size: 800, n_layers: 1, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.964  0.695  0.758  0.792  0.478  0.667  0.526         0.889\n",
      "test, Precision  0.955  0.684  0.733  0.802  0.604  0.629  0.465         0.868\n",
      "dev, Recall      0.941  0.837  0.862  0.711  1.000  0.704  0.649         0.889\n",
      "test, Recall     0.944  0.801  0.770  0.659  0.914  0.657  0.567         0.868\n",
      "\n",
      "Model 8 , hidden_size: 800, n_layers: 2, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.979  0.710  0.745  0.732  0.391  0.637  0.491         0.913\n",
      "test, Precision  0.983  0.714  0.747  0.749  0.509  0.629  0.510         0.918\n",
      "dev, Recall      0.930  0.821  0.900  0.870  0.900  0.817  0.648         0.913\n",
      "test, Recall     0.936  0.833  0.877  0.848  0.931  0.837  0.773         0.918\n",
      "\n",
      "Model 9 , hidden_size: 800, n_layers: 3, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.970  0.765  0.771  0.743  0.522  0.631  0.466         0.862\n",
      "test, Precision  0.965  0.781  0.757  0.755  0.642  0.614  0.380         0.868\n",
      "dev, Recall      0.946  0.607  0.818  0.877  0.706  0.785  0.885         0.862\n",
      "test, Recall     0.947  0.629  0.789  0.881  0.810  0.757  0.691         0.868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(vocab=_vocab, load_glove_weights=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (GloVe), hidden_size: 500, n_layers: 1, directions: 1)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.952  0.545  0.828  0.628  0.478  0.613  0.293         0.742\n",
      "test, Precision  0.952  0.491  0.811  0.633  0.623  0.646  0.340         0.722\n",
      "dev, Recall      0.945  0.801  0.602  0.752  0.550  0.414  0.654         0.742\n",
      "test, Recall     0.955  0.813  0.562  0.689  0.508  0.425  0.708         0.722\n",
      "\n",
      "Model 2 (GloVe), hidden_size: 500, n_layers: 1, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.984  0.680  0.803  0.639  0.348  0.595  0.569         0.835\n",
      "test, Precision  0.975  0.622  0.736  0.662  0.377  0.614  0.595         0.839\n",
      "dev, Recall      0.943  0.866  0.926  0.755  0.444  0.671  0.688         0.835\n",
      "test, Recall     0.941  0.854  0.897  0.767  0.541  0.683  0.513         0.839\n",
      "\n",
      "Model 3 (GloVe), hidden_size: 500, n_layers: 2, directions: 1)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.943  0.545  0.854  0.634  0.522  0.649  0.379         0.698\n",
      "test, Precision  0.944  0.521  0.875  0.621  0.472  0.683  0.405         0.704\n",
      "dev, Recall      0.968  0.669  0.556  0.773  0.414  0.399  0.603         0.698\n",
      "test, Recall     0.970  0.687  0.545  0.727  0.521  0.414  0.633         0.704\n",
      "\n",
      "Model 4 (GloVe), hidden_size: 500, n_layers: 2, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.969  0.875  0.949  0.765  0.478  0.589  0.422         0.833\n",
      "test, Precision  0.966  0.816  0.902  0.752  0.585  0.640  0.575         0.851\n",
      "dev, Recall      0.968  0.738  0.784  0.729  0.500  0.692  0.803         0.833\n",
      "test, Recall     0.968  0.756  0.776  0.694  0.660  0.759  0.701         0.851\n",
      "\n",
      "Model 5 (GloVe), hidden_size: 500, n_layers: 3, directions: 1)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.943  0.555  0.879  0.639  0.348  0.673  0.362         0.710\n",
      "test, Precision  0.940  0.544  0.872  0.653  0.415  0.697  0.320         0.696\n",
      "dev, Recall      0.966  0.703  0.541  0.760  0.800  0.399  0.677         0.710\n",
      "test, Recall     0.973  0.702  0.527  0.704  0.733  0.399  0.577         0.696\n",
      "\n",
      "Model 6 (GloVe), hidden_size: 500, n_layers: 3, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.982  0.820  0.911  0.705  0.391  0.565  0.474         0.864\n",
      "test, Precision  0.980  0.749  0.845  0.697  0.623  0.617  0.540         0.862\n",
      "dev, Recall      0.951  0.804  0.856  0.801  0.562  0.766  0.743         0.864\n",
      "test, Recall     0.953  0.778  0.887  0.738  0.702  0.803  0.720         0.862\n",
      "\n",
      "Model 7 (GloVe), hidden_size: 800, n_layers: 1, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.974  0.720  0.847  0.760  0.391  0.577  0.474         0.822\n",
      "test, Precision  0.968  0.684  0.841  0.723  0.415  0.654  0.555         0.831\n",
      "dev, Recall      0.954  0.766  0.826  0.732  0.529  0.642  0.733         0.822\n",
      "test, Recall     0.957  0.825  0.871  0.683  0.595  0.617  0.600         0.831\n",
      "\n",
      "Model 8 (GloVe), hidden_size: 800, n_layers: 2, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.974  0.790  0.860  0.656  0.522  0.661  0.603         0.819\n",
      "test, Precision  0.973  0.737  0.828  0.694  0.717  0.711  0.605         0.835\n",
      "dev, Recall      0.966  0.782  0.877  0.805  0.333  0.631  0.673         0.819\n",
      "test, Recall     0.966  0.802  0.860  0.799  0.384  0.662  0.699         0.835\n",
      "\n",
      "Model 9 (GloVe), hidden_size: 800, n_layers: 3, directions: 2)\n",
      "                     O  B-PER  I-PER  B-LOC  I-LOC  B-ORG  I-ORG  All Except O\n",
      "dev, Precision   0.974  0.830  0.892  0.639  0.348  0.613  0.466         0.809\n",
      "test, Precision  0.975  0.772  0.851  0.647  0.453  0.706  0.630         0.838\n",
      "dev, Recall      0.962  0.738  0.824  0.801  0.500  0.599  0.692         0.809\n",
      "test, Recall     0.964  0.783  0.857  0.760  0.686  0.675  0.681         0.838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(vocab=_vocab, load_glove_weights=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}